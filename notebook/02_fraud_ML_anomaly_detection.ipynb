{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82931f95-e914-44a5-b2a5-66b77503b948",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#02_fraud_ML_anomaly_detection notebook\n",
    "# ML Anomaly detection using Spark ML \n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col, current_date, date_format\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # =====================================================\n",
    "        # 1. Read and validate data\n",
    "        # =====================================================\n",
    "        print(\"Reading claims data for ML anomaly detection...\")\n",
    "        claims_data = spark.table(\"medisure_jen.gold.gold_claims_analytics\")\n",
    "        spark.sql(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS medisure_jen.audit.fraud_monitoring_log (\n",
    "            check_timestamp TIMESTAMP,\n",
    "            critical_alerts INT,\n",
    "            ml_anomalies INT,\n",
    "            providers_needing_review INT,\n",
    "            email_sent BOOLEAN,\n",
    "            data_freshness_hours DOUBLE,\n",
    "            volume_status STRING\n",
    "        )\n",
    "        \"\"\")\n",
    "        # Check if data is available\n",
    "        if claims_data.count() == 0:\n",
    "            print(\"No claims data available for analysis\")\n",
    "            spark.sql(\"CREATE SCHEMA IF NOT EXISTS medisure_jen.temp\")\n",
    "            spark.createDataFrame([], [\"claim_id\"]).write.mode(\"overwrite\")\\\n",
    "                .saveAsTable(\"medisure_jen.temp.ml_anomalies\")\n",
    "            return 0\n",
    "        \n",
    "        print(f\"Processing {claims_data.count()} claims for anomaly detection\")\n",
    "        \n",
    "        # Display sample data for monitoring\n",
    "        print(\"Sample claims data for ML analysis:\")\n",
    "        display(claims_data.select(\"claim_id\", \"claim_amount\", \"fraud_risk_score\", \"diagnosis_code\", \"provider_id\").limit(10))\n",
    "        \n",
    "        # =====================================================\n",
    "        # 2. ML Anomaly Detection\n",
    "        # =====================================================\n",
    "        print(\"Running KMeans clustering for anomaly detection...\")\n",
    "        \n",
    "        assembler = VectorAssembler(inputCols=[\"claim_amount\", \"fraud_risk_score\"], outputCol=\"features\")\n",
    "        kmeans = KMeans(k=3, seed=42)\n",
    "        pipeline = Pipeline(stages=[assembler, kmeans])\n",
    "        model = pipeline.fit(claims_data.limit(10000))\n",
    "        \n",
    "        results = model.transform(claims_data)\n",
    "        anomalies = results.filter(col(\"prediction\") == 2)\n",
    "        \n",
    "        # =====================================================\n",
    "        # 3. Save and display results\n",
    "        # =====================================================\n",
    "        anomaly_count = anomalies.count()\n",
    "        print(f\"Detected {anomaly_count} anomalous claims using ML\")\n",
    "        \n",
    "        if anomaly_count > 0:\n",
    "            print(\"Top anomalous claims:\")\n",
    "            display(anomalies.select(\"claim_id\", \"claim_amount\", \"fraud_risk_score\", \"prediction\", \"diagnosis_code\", \"provider_id\").limit(10))\n",
    "        \n",
    "        # Save for potential alerts\n",
    "        spark.sql(\"CREATE SCHEMA IF NOT EXISTS medisure_jen.temp\")\n",
    "        anomalies.write.mode(\"overwrite\").saveAsTable(\"medisure_jen.temp.ml_anomalies\")\n",
    "        \n",
    "        # =====================================================\n",
    "        # 4. Additional monitoring: Recent anomalies trend\n",
    "        # =====================================================\n",
    "        try:\n",
    "            recent_anomalies_trend = spark.sql(f\"\"\"\n",
    "            SELECT \n",
    "                date_format(processing_date, 'yyyy-MM') as month,\n",
    "                COUNT(*) as total_claims,\n",
    "                SUM(CASE WHEN fraud_risk_score > 0.7 THEN 1 ELSE 0 END) as high_risk_claims,\n",
    "                ROUND(AVG(fraud_risk_score), 3) as avg_fraud_score\n",
    "            FROM medisure_jen.gold.gold_claims_analytics\n",
    "            WHERE processing_date >= add_months(current_date(), -6)\n",
    "            GROUP BY date_format(processing_date, 'yyyy-MM')\n",
    "            ORDER BY month\n",
    "            \"\"\")\n",
    "            \n",
    "            print(\"Recent fraud trends for context:\")\n",
    "            display(recent_anomalies_trend)\n",
    "            \n",
    "        except Exception as trend_error:\n",
    "            print(f\"Trend analysis skipped: {trend_error}\")\n",
    "        \n",
    "        # =====================================================\n",
    "        # 5. Summary output\n",
    "        # =====================================================\n",
    "        print(\"=\"*60)\n",
    "        print(\"ML ANOMALY DETECTION SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Total Claims Processed: {claims_data.count()}\")\n",
    "        print(f\"Anomalous Claims Detected: {anomaly_count}\")\n",
    "        print(f\"Anomaly Rate: {(anomaly_count/claims_data.count()*100 if claims_data.count() > 0 else 0):.2f}%\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        return anomaly_count\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ML anomaly detection failed: {e}\")\n",
    "        # Create empty table structure for downstream dependencies\n",
    "        spark.sql(\"CREATE SCHEMA IF NOT EXISTS medisure_jen.temp\")\n",
    "        empty_df = spark.createDataFrame([], \"claim_id string, claim_amount double, fraud_risk_score double, prediction int, features array<double>\")\n",
    "        empty_df.write.mode(\"overwrite\").saveAsTable(\"medisure_jen.temp.ml_anomalies\")\n",
    "        return 0\n",
    "\n",
    "# Execute main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_fraud_ML_anomaly_detection",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
