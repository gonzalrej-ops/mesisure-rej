{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a68c3726-5a72-4bf3-aa47-42163dfbdff0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Purpose: Orchestrate the ingestion of files from Git Repo to both Volume and DBFS paths.\n",
    "#Step 1: Fetch Latest Files from GitHub\n",
    "\n",
    "import requests\n",
    "\n",
    "# GitHub configuration\n",
    "github_repo_raw_url = \"https://raw.githubusercontent.com/jen-mejarito/medisure_jen/main/data/\"\n",
    "file_names = [\"claims_batch.csv\", \"claims_stream.json\", \"diagnosis_ref.csv\", \"members.csv\", \"providers.json\"]\n",
    "\n",
    "# Target landing zones\n",
    "volume_landing_path = \"/Volumes/medisure_jen/bronze/landing_zone/\" #batch processing\n",
    "dbfs_landing_path = \"/Volumes/medisure_jen/bronze/autoloader_landing/\" # for autoloadeer\n",
    "\n",
    "# Create the DBFS directory if it doesn't exist\n",
    "dbutils.fs.mkdirs(dbfs_landing_path)\n",
    "print(f\"Ensuring DBFS path exists: {dbfs_landing_path}\")\n",
    "\n",
    "#Step 2: Copy Each File from Git to Both Locations\n",
    "for file_name in file_names:\n",
    "    file_url = f\"{github_repo_raw_url}{file_name}\"\n",
    "\n",
    "    print(f\"Downloading {file_name} from Git...\")\n",
    "    try:\n",
    "        response = requests.get(file_url)\n",
    "        response.raise_for_status()  # Raises an exception for HTTP errors \n",
    "        file_content = response.text\n",
    "        \n",
    "        # 1. Write to Volume (for batch processing)\n",
    "        volume_target_path = f\"{volume_landing_path}{file_name}\"\n",
    "        print(f\"Writing {file_name} to Volume: {volume_target_path}\")\n",
    "        dbutils.fs.put(volume_target_path, file_content)\n",
    "        \n",
    "        # 2. If it's the streaming file, ALSO write to DBFS (for Auto Loader)\n",
    "        if file_name == \"claims_stream.json\":\n",
    "            dbfs_target_path = f\"{dbfs_landing_path}{file_name}\"\n",
    "            print(f\"Writing {file_name} to DBFS for Auto Loader: {dbfs_target_path}\")\n",
    "            dbutils.fs.put(dbfs_target_path, file_content)\n",
    "        \n",
    "        print(f\"Successfully transferred {file_name}\\n\")\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Failed to download {file_name}: {e}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred with {file_name}: {e}\\n\")\n",
    "\n",
    "print(\"All files processed from GitHub.\")\n",
    "\n",
    "#Step 3: Verify Files in Both Locations\n",
    "print(\"Files in Volume landing zone (/Volumes/...):\")\n",
    "display(dbutils.fs.ls(volume_landing_path))\n",
    "\n",
    "print(\"\\nFiles in DBFS landing zone (dbfs:/tmp/...) - for Auto Loader:\")\n",
    "display(dbutils.fs.ls(dbfs_landing_path))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "0_ingestion_git_to_databricks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
