{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89c3fddf-b668-4d01-a6c7-7de8fa45e700",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 03 - Fraud Monitoring & Alert System\n",
    "# **Purpose**: Monitor fraud patterns, generate alerts, and create compliance reports\n",
    "# **Schedule**: Run daily after Gold layer processing completes\n",
    "\n",
    "from pyspark.sql.functions import (\n",
    "    col, current_timestamp, split, when, lit, date_format,\n",
    "    add_months, expr, size, unix_timestamp, datediff, hour\n",
    ")\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml import Pipeline\n",
    "from datetime import datetime, date\n",
    "import time\n",
    "\n",
    "# =====================================================\n",
    "# 1. Read from Gold tables\n",
    "# =====================================================\n",
    "gold_claims = spark.table(\"medisure_jen.gold.gold_claims_analytics\")\n",
    "fraud_alerts = spark.table(\"medisure_jen.gold.gold_realtime_fraud_alerts\")\n",
    "provider_performance = spark.table(\"medisure_jen.gold.gold_provider_performance\")\n",
    "member_summary = spark.table(\"medisure_jen.gold.gold_member_claims_summary\")\n",
    "\n",
    "# Display current day's fraud alerts\n",
    "display(spark.sql(\"\"\"\n",
    "SELECT * FROM medisure_jen.gold.gold_realtime_fraud_alerts \n",
    "ORDER BY alert_severity DESC, alert_timestamp DESC\n",
    "\"\"\"))\n",
    "\n",
    "# =====================================================\n",
    "# 1.2 Fraud Trends Analysis\n",
    "# =====================================================\n",
    "fraud_trends = spark.sql(\"\"\"\n",
    "SELECT \n",
    "  processing_month,\n",
    "  COUNT(*) as total_claims,\n",
    "  SUM(CASE WHEN fraud_risk_score > 0.7 THEN 1 ELSE 0 END) as high_risk_claims,\n",
    "  ROUND(AVG(fraud_risk_score), 3) as avg_fraud_score,\n",
    "  ROUND(SUM(CASE WHEN fraud_risk_score > 0.7 THEN claim_amount ELSE 0 END), 2) as high_risk_amount\n",
    "FROM medisure_jen.gold.gold_claims_analytics\n",
    "WHERE processing_month >= date_format(add_months(current_date(), -6), 'yyyy-MM')\n",
    "GROUP BY processing_month\n",
    "ORDER BY processing_month\n",
    "\"\"\")\n",
    "display(fraud_trends)\n",
    "\n",
    "# =====================================================\n",
    "# 2. Compliance Reporting\n",
    "# =====================================================\n",
    "# 2.1 Provider Compliance Report\n",
    "provider_compliance_report = spark.sql(\"\"\"\n",
    "SELECT \n",
    "  provider_id,\n",
    "  provider_name,\n",
    "  tin,\n",
    "  total_claims,\n",
    "  total_amount,\n",
    "  avg_claim_amount,\n",
    "  avg_fraud_score,\n",
    "  high_risk_claims,\n",
    "  ROUND((high_risk_claims / total_claims) * 100, 2) as high_risk_percentage,\n",
    "  CASE \n",
    "    WHEN (high_risk_claims / total_claims) > 0.3 THEN 'REVIEW REQUIRED'\n",
    "    WHEN (high_risk_claims / total_claims) > 0.1 THEN 'MONITOR'\n",
    "    ELSE 'COMPLIANT'\n",
    "  END as compliance_status\n",
    "FROM medisure_jen.gold.gold_provider_performance\n",
    "WHERE reporting_period = date_format(current_date(), 'yyyy-MM')\n",
    "ORDER BY high_risk_percentage DESC\n",
    "\"\"\")\n",
    "display(provider_compliance_report)\n",
    "\n",
    "# 2.2 Member Risk Profiling\n",
    "member_risk_profiles = spark.sql(\"\"\"\n",
    "SELECT \n",
    "  member_id,\n",
    "  first_name,\n",
    "  last_name,\n",
    "  claims_count,\n",
    "  total_claimed,\n",
    "  member_risk_score,\n",
    "  CASE \n",
    "    WHEN member_risk_score > 0.8 THEN 'HIGH RISK'\n",
    "    WHEN member_risk_score > 0.5 THEN 'MEDIUM RISK'\n",
    "    ELSE 'LOW RISK'\n",
    "  END as risk_category\n",
    "FROM medisure_jen.gold.gold_member_claims_summary\n",
    "WHERE summary_period = date_format(current_date(), 'yyyy-MM-dd')\n",
    "ORDER BY member_risk_score DESC\n",
    "LIMIT 100\n",
    "\"\"\")\n",
    "display(member_risk_profiles)\n",
    "\n",
    "# =====================================================\n",
    "# 3. Alert Generation & Notification\n",
    "# =====================================================\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "@udf(FloatType())\n",
    "def advanced_fraud_score(amount, diagnosis_code, provider_id, claim_frequency, member_risk_history):\n",
    "    score = 0.0\n",
    "    \n",
    "    # Amount-based scoring (progressive)\n",
    "    try:\n",
    "        amount_val = float(amount) if amount else 0\n",
    "        if amount_val > 50000: score += 0.4\n",
    "        elif amount_val > 25000: score += 0.3\n",
    "        elif amount_val > 10000: score += 0.2\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Diagnosis code patterns\n",
    "    suspicious_diagnoses = ['E119', 'I10', 'M545', 'R558', 'Z798']\n",
    "    if diagnosis_code in suspicious_diagnoses:\n",
    "        score += 0.2\n",
    "    \n",
    "    # Provider watchlist\n",
    "    high_risk_providers = ['PROV999', 'PROV888', 'PROV777']\n",
    "    if provider_id in high_risk_providers:\n",
    "        score += 0.3\n",
    "    \n",
    "    # Claim frequency\n",
    "    claim_freq = float(claim_frequency) if claim_frequency else 0\n",
    "    if claim_freq > 10:\n",
    "        score += 0.2\n",
    "    \n",
    "    # Member history\n",
    "    member_risk = float(member_risk_history) if member_risk_history else 0\n",
    "    if member_risk > 0.6:\n",
    "        score += 0.2\n",
    "    \n",
    "    return min(score, 1.0)\n",
    "\n",
    "# Apply enhanced fraud scoring\n",
    "alerts_with_enhanced_scoring = spark.sql(\"\"\"\n",
    "SELECT \n",
    "  a.claim_id,\n",
    "  a.member_id,\n",
    "  a.provider_id,\n",
    "  a.claim_amount,\n",
    "  a.diagnosis_code,\n",
    "  a.alert_severity,\n",
    "  a.alert_reason,\n",
    "  a.alert_timestamp,\n",
    "  split(m.member_name, ' ')[0] as first_name,\n",
    "  array_join(slice(split(m.member_name, ' '), 2, size(split(m.member_name, ' ')) - 1), ' ') as last_name,\n",
    "  p.provider_name,\n",
    "  mem.claims_count as member_claim_frequency,\n",
    "  mem.member_risk_score as member_risk_history\n",
    "FROM medisure_jen.gold.gold_realtime_fraud_alerts a\n",
    "LEFT JOIN medisure_jen.silver.silver_members m ON a.member_id = m.member_id\n",
    "LEFT JOIN medisure_jen.silver.silver_providers p ON a.provider_id = p.provider_id\n",
    "LEFT JOIN medisure_jen.gold.gold_member_claims_summary mem ON a.member_id = mem.member_id \n",
    "  AND mem.summary_period = date_format(current_date(), 'yyyy-MM-dd')\n",
    "WHERE a.alert_severity IN ('Critical', 'High')\n",
    "  AND date(a.alert_timestamp) >= date_add(current_date(), -7)\n",
    "\"\"\")\n",
    "\n",
    "# Register UDF and apply enhanced scoring\n",
    "spark.udf.register(\"advanced_fraud_score\", advanced_fraud_score)\n",
    "\n",
    "critical_alerts = alerts_with_enhanced_scoring.withColumn(\n",
    "    \"enhanced_fraud_score\", \n",
    "    advanced_fraud_score(\n",
    "        col(\"claim_amount\"), \n",
    "        col(\"diagnosis_code\"), \n",
    "        col(\"provider_id\"),\n",
    "        col(\"member_claim_frequency\"),\n",
    "        col(\"member_risk_history\")\n",
    "    )\n",
    ").filter(col(\"enhanced_fraud_score\") >= 0.5).orderBy(col(\"claim_amount\").desc())\n",
    "\n",
    "critical_alerts.createOrReplaceTempView(\"critical_alerts\")\n",
    "display(critical_alerts)\n",
    "\n",
    "# 3.2 ML-based Anomaly Detection\n",
    "def detect_anomalies():\n",
    "    try:\n",
    "        claims_data = spark.table(\"medisure_jen.gold.gold_claims_analytics\")\n",
    "        \n",
    "        assembler = VectorAssembler(inputCols=[\"claim_amount\", \"fraud_risk_score\"], outputCol=\"features\")\n",
    "        kmeans = KMeans(k=3, seed=42)\n",
    "        pipeline = Pipeline(stages=[assembler, kmeans])\n",
    "        model = pipeline.fit(claims_data.limit(10000))\n",
    "        \n",
    "        results = model.transform(claims_data)\n",
    "        anomalies = results.filter(col(\"prediction\") == 2)\n",
    "        \n",
    "        print(f\"Detected {anomalies.count()} anomalous claims using ML\")\n",
    "        return anomalies\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ML anomaly detection failed: {e}\")\n",
    "        return spark.createDataFrame([], claims_data.schema)\n",
    "\n",
    "anomalous_claims = detect_anomalies()\n",
    "if anomalous_claims.count() > 0:\n",
    "    display(anomalous_claims.select(\"claim_id\", \"claim_amount\", \"fraud_risk_score\", \"prediction\"))\n",
    "\n",
    "# 3.3 Email Alert Function\n",
    "def send_fraud_alert_email(alert_data):\n",
    "    critical_count = alert_data.count()\n",
    "    \n",
    "    if critical_count > 0:\n",
    "        subject = f\"ðŸš¨ MediSure Fraud Alert: {critical_count} Critical Cases Detected\"\n",
    "        message = f\"\"\"\n",
    "        <h3>Critical Fraud Alerts - {date.today()}</h3>\n",
    "        <p>Number of critical alerts: <strong>{critical_count}</strong></p>\n",
    "        <p>ML Anomalies Detected: <strong>{anomalous_claims.count()}</strong></p>\n",
    "        <h4>Top 5 Critical Cases:</h4>\n",
    "        <table border='1'>\n",
    "        <tr>\n",
    "            <th>Claim ID</th>\n",
    "            <th>Member</th>\n",
    "            <th>Provider</th>\n",
    "            <th>Amount</th>\n",
    "            <th>Fraud Score</th>\n",
    "            <th>Reason</th>\n",
    "        </tr>\n",
    "        \"\"\"\n",
    "        for row in alert_data.limit(5).collect():\n",
    "            message += f\"\"\"\n",
    "            <tr>\n",
    "                <td>{row.claim_id}</td>\n",
    "                <td>{row.first_name or \"Unknown\"} {row.last_name or \"\"}</td>\n",
    "                <td>{row.provider_name or \"Unknown Provider\"}</td>\n",
    "                <td>${float(row.claim_amount or 0.0):,.2f}</td>\n",
    "                <td>{float(getattr(row, 'enhanced_fraud_score', 0.0)):.2f}</td>\n",
    "                <td>{row.alert_reason or \"Unknown\"}</td>\n",
    "            </tr>\n",
    "            \"\"\"\n",
    "        message += \"</table>\"\n",
    "        print(f\"Would send email with subject: {subject}\")\n",
    "        print(f\"To: icon.montalbar@gmail.com\")\n",
    "        print(f\"Body: {message}\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"No critical alerts to report today.\")\n",
    "        return False\n",
    "\n",
    "email_sent = send_fraud_alert_email(critical_alerts)\n",
    "\n",
    "# 3.4 Save Enhanced Compliance Reports with RBAC\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TABLE medisure_jen.audit.fraud_alerts_restricted\n",
    "AS SELECT \n",
    "  claim_id,\n",
    "  provider_id,\n",
    "  claim_amount,\n",
    "  alert_severity,\n",
    "  enhanced_fraud_score,\n",
    "  alert_reason,\n",
    "  CASE \n",
    "    WHEN current_user() LIKE '%compliance%' THEN member_id\n",
    "    ELSE 'REDACTED'\n",
    "  END as member_id,\n",
    "  alert_timestamp\n",
    "FROM critical_alerts\n",
    "\"\"\")\n",
    "\n",
    "(provider_compliance_report.write.format(\"delta\").mode(\"overwrite\")\n",
    " .option(\"mergeSchema\", \"true\").saveAsTable(\"medisure_jen.audit.provider_compliance_daily\"))\n",
    "\n",
    "(member_risk_profiles.write.format(\"delta\").mode(\"overwrite\")\n",
    " .option(\"mergeSchema\", \"true\").saveAsTable(\"medisure_jen.audit.member_risk_daily\"))\n",
    "\n",
    "if anomalous_claims.count() > 0:\n",
    "    (anomalous_claims.write.format(\"delta\").mode(\"overwrite\")\n",
    "     .option(\"mergeSchema\", \"true\").saveAsTable(\"medisure_jen.audit.ml_anomalies_daily\"))\n",
    "\n",
    "# =====================================================\n",
    "# 4. Operational Monitoring\n",
    "# =====================================================\n",
    "# 4.0 Ensure monitoring log table exists\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS medisure_jen.audit.fraud_monitoring_log (\n",
    "    check_timestamp TIMESTAMP,\n",
    "    critical_alerts INT,\n",
    "    ml_anomalies INT,\n",
    "    providers_needing_review INT,\n",
    "    email_sent BOOLEAN,\n",
    "    data_freshness_hours DOUBLE,\n",
    "    volume_status STRING\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "# 4.1 Pipeline Health Check\n",
    "def monitor_pipeline_health():\n",
    "    # Check data freshness\n",
    "    freshness = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "      MAX(alert_timestamp) as latest_alert,\n",
    "      current_timestamp() as current_time,\n",
    "      ROUND((unix_timestamp(current_timestamp()) - unix_timestamp(MAX(alert_timestamp))) / 3600, 2) as hours_diff\n",
    "    FROM medisure_jen.gold.gold_realtime_fraud_alerts\n",
    "    \"\"\")\n",
    "    \n",
    "    # Check today's count\n",
    "    today_count_df = spark.sql(\"\"\"\n",
    "    SELECT COUNT(*) as current_volume\n",
    "    FROM medisure_jen.gold.gold_realtime_fraud_alerts\n",
    "    WHERE date(alert_timestamp) = current_date()\n",
    "    \"\"\")\n",
    "    today_count = today_count_df.first().current_volume if today_count_df.count() > 0 else 0\n",
    "    \n",
    "    # Get historical average\n",
    "    historical_avg_df = spark.sql(\"\"\"\n",
    "    SELECT COALESCE(AVG(daily_count), 0) as historical_avg\n",
    "    FROM (\n",
    "      SELECT date(check_timestamp), COUNT(*) as daily_count\n",
    "      FROM medisure_jen.audit.fraud_monitoring_log\n",
    "      GROUP BY date(check_timestamp)\n",
    "    )\n",
    "    \"\"\")\n",
    "    historical_avg = historical_avg_df.first().historical_avg if historical_avg_df.count() > 0 else 0\n",
    "    \n",
    "    volume_status = \"HIGH_VOLUME_ALERT\" if today_count > historical_avg * 2 else \"NORMAL\"\n",
    "    \n",
    "    volume_stats = spark.createDataFrame([(\n",
    "        today_count,\n",
    "        historical_avg,\n",
    "        volume_status\n",
    "    )], [\"current_volume\", \"historical_avg\", \"volume_status\"])\n",
    "    \n",
    "    return freshness, volume_stats\n",
    "\n",
    "# CALL THE FUNCTION AND STORE RESULTS\n",
    "freshness_check, volume_check = monitor_pipeline_health()\n",
    "\n",
    "# Display the results\n",
    "display(freshness_check)\n",
    "display(volume_check)\n",
    "\n",
    "# 4.2 Time Travel Audit\n",
    "def audit_claim_changes():\n",
    "    try:\n",
    "        sample_claim = critical_alerts.select(\"claim_id\").first()\n",
    "        if sample_claim:\n",
    "            claim_id = sample_claim.claim_id\n",
    "            history = spark.sql(f\"\"\"\n",
    "            DESCRIBE HISTORY medisure_jen.silver.silver_claims \n",
    "            WHERE claim_id = '{claim_id}' OR claim_id IS not NULL\n",
    "            LIMIT 5\n",
    "            \"\"\")\n",
    "            if history.count() > 0:\n",
    "                display(history)\n",
    "            else:\n",
    "                print(\"No history available for sample claims\")\n",
    "            return history\n",
    "    except Exception as e:\n",
    "        print(f\"Audit functionality not available: {e}\")\n",
    "        return None\n",
    "\n",
    "claim_history = audit_claim_changes()\n",
    "\n",
    "# 4.3 Log Monitoring Results - SAFE DATAFRAME APPROACH\n",
    "from pyspark.sql.types import StructType, StructField, TimestampType, IntegerType, BooleanType, DoubleType, StringType\n",
    "import datetime\n",
    "\n",
    "# Define explicit schema\n",
    "monitoring_schema = StructType([\n",
    "    StructField(\"check_timestamp\", TimestampType(), True),\n",
    "    StructField(\"critical_alerts\", IntegerType(), True),\n",
    "    StructField(\"ml_anomalies\", IntegerType(), True),\n",
    "    StructField(\"providers_needing_review\", IntegerType(), True),\n",
    "    StructField(\"email_sent\", BooleanType(), True),\n",
    "    StructField(\"data_freshness_hours\", DoubleType(), True),\n",
    "    StructField(\"volume_status\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Extract values safely with defaults\n",
    "freshness_hours = freshness_check.select(\"hours_diff\").first()[0] if freshness_check.count() > 0 else 0.0\n",
    "volume_status = volume_check.select(\"volume_status\").first()[0] if volume_check.count() > 0 else \"UNKNOWN\"\n",
    "\n",
    "# Create row with safe values\n",
    "monitoring_data = [\n",
    "    (\n",
    "        datetime.datetime.now(),  # Use Python datetime\n",
    "        int(critical_alerts.count() or 0),\n",
    "        int(anomalous_claims.count() or 0),\n",
    "        int(provider_compliance_report.filter(col(\"compliance_status\") != \"COMPLIANT\").count() or 0),\n",
    "        bool(email_sent),\n",
    "        float(freshness_hours or 0.0),\n",
    "        str(volume_status or \"UNKNOWN\")\n",
    "    )\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "monitoring_log = spark.createDataFrame(monitoring_data, monitoring_schema)\n",
    "\n",
    "# Save to audit table\n",
    "(monitoring_log.write.format(\"delta\").mode(\"append\")\n",
    " .option(\"mergeSchema\", \"true\")\n",
    " .saveAsTable(\"medisure_jen.audit.fraud_monitoring_log\"))\n",
    "\n",
    "# =====================================================\n",
    "# 5. Summary Output\n",
    "# =====================================================\n",
    "print(\"=\"*80)\n",
    "print(\"FRAUD MONITORING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Execution Time: {datetime.datetime.now()}\")\n",
    "print(f\"Critical Alerts Found: {critical_alerts.count()}\")\n",
    "print(f\"ML Anomalies Detected: {anomalous_claims.count()}\")\n",
    "print(f\"Providers Needing Review: {provider_compliance_report.filter(col('compliance_status') != 'COMPLIANT').count()}\")\n",
    "print(f\"Email Alert Sent: {'Yes' if email_sent else 'No'}\")\n",
    "print(f\"Data Freshness: {freshness_hours} hours\")\n",
    "print(f\"Volume Status: {volume_status}\")\n",
    "print(\"=\"*80)\n",
    "print(\"Capstone Requirements:\")\n",
    "print(\"âœ… Real-time fraud detection with enhanced scoring\")\n",
    "print(\"âœ… ML-based anomaly detection\")\n",
    "print(\"âœ… Unity Catalog RBAC implementation\")\n",
    "print(\"âœ… Delta Lake time travel auditing\")\n",
    "print(\"âœ… Advanced monitoring and alerting\")\n",
    "print(\"âœ… Data quality and governance\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "fraud_monitoring",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
