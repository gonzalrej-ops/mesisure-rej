{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10be1f03-3dfb-4675-998d-e46b6ded2846",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 06_audit_logging.py\n",
    "# Audit trail for compliance purpose and final clean up\n",
    "from pyspark.sql.functions import current_timestamp, col\n",
    "from pyspark.sql.types import StructType, StructField, TimestampType, IntegerType, BooleanType, DoubleType, StringType\n",
    "import datetime\n",
    "\n",
    "def monitor_pipeline_health():\n",
    "    try:\n",
    "        freshness = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "          MAX(alert_timestamp) as latest_alert,\n",
    "          current_timestamp() as current_time,\n",
    "          ROUND((unix_timestamp(current_timestamp()) - unix_timestamp(MAX(alert_timestamp))) / 3600, 2) as hours_diff\n",
    "        FROM medisure_jen.gold.gold_realtime_fraud_alerts\n",
    "        \"\"\")\n",
    "        \n",
    "        today_count_df = spark.sql(\"\"\"\n",
    "        SELECT COUNT(*) as current_volume\n",
    "        FROM medisure_jen.gold.gold_realtime_fraud_alerts\n",
    "        WHERE date(alert_timestamp) = current_date()\n",
    "        \"\"\")\n",
    "        today_count = today_count_df.first().current_volume if today_count_df.count() > 0 else 0\n",
    "        \n",
    "        historical_avg_df = spark.sql(\"\"\"\n",
    "        SELECT COALESCE(AVG(daily_count), 0) as historical_avg\n",
    "        FROM (\n",
    "          SELECT date(check_timestamp), COUNT(*) as daily_count\n",
    "          FROM medisure_jen.audit.fraud_monitoring_log\n",
    "          GROUP BY date(check_timestamp)\n",
    "        )\n",
    "        \"\"\")\n",
    "        historical_avg = historical_avg_df.first().historical_avg if historical_avg_df.count() > 0 else 0\n",
    "        \n",
    "        volume_status = \"HIGH_VOLUME_ALERT\" if today_count > historical_avg * 2 else \"NORMAL\"\n",
    "        \n",
    "        return freshness, today_count, historical_avg, volume_status\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Pipeline health monitoring failed: {e}\")\n",
    "        # Return default values\n",
    "        return spark.createDataFrame([(0.0,)], [\"hours_diff\"]), 0, 0, \"UNKNOWN\"\n",
    "\n",
    "def main():\n",
    "    print(\"=\"*60)\n",
    "    print(\"AUDIT LOGGING STARTED\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 1. Get monitoring metrics with error handling\n",
    "    try:\n",
    "        freshness_check, current_volume, historical_avg, volume_status = monitor_pipeline_health()\n",
    "        freshness_hours = (\n",
    "            freshness_check.select(\"hours_diff\").first()[0]\n",
    "            if freshness_check.count() > 0 else 0.0\n",
    "        )\n",
    "        if freshness_hours is None:\n",
    "            freshness_hours = 0.0\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting monitoring metrics: {e}\")\n",
    "        freshness_hours, current_volume, historical_avg, volume_status = 0.0, 0, 0, \"ERROR\"\n",
    "    \n",
    "    # 2. Get counts from previous tasks with error handling\n",
    "    try:\n",
    "        critical_count = spark.table(\"medisure_jen.temp.critical_alerts\").count()\n",
    "    except:\n",
    "        critical_count = 0\n",
    "        print(\"Warning: critical_alerts table not found\")\n",
    "    \n",
    "    try:\n",
    "        ml_count = spark.table(\"medisure_jen.temp.ml_anomalies\").count()\n",
    "    except:\n",
    "        ml_count = 0\n",
    "        print(\"Warning: ml_anomalies table not found\")\n",
    "    \n",
    "    try:\n",
    "        email_sent = spark.table(\"medisure_jen.temp.email_status\").first().email_sent\n",
    "    except:\n",
    "        email_sent = False\n",
    "        print(\"Warning: email_status table not found\")\n",
    "    \n",
    "    try:\n",
    "        providers_review = spark.table(\"medisure_jen.audit.provider_compliance_daily\")\\\n",
    "            .filter(col(\"compliance_status\") != \"COMPLIANT\").count()\n",
    "    except:\n",
    "        providers_review = 0\n",
    "        print(\"Warning: provider_compliance_daily table not found\")\n",
    "    \n",
    "    # 3. Create audit log\n",
    "    monitoring_schema = StructType([\n",
    "        StructField(\"check_timestamp\", TimestampType(), True),\n",
    "        StructField(\"critical_alerts\", IntegerType(), True),\n",
    "        StructField(\"ml_anomalies\", IntegerType(), True),\n",
    "        StructField(\"providers_needing_review\", IntegerType(), True),\n",
    "        StructField(\"email_sent\", BooleanType(), True),\n",
    "        StructField(\"data_freshness_hours\", DoubleType(), True),\n",
    "        StructField(\"volume_status\", StringType(), True),\n",
    "        StructField(\"current_volume\", IntegerType(), True),\n",
    "        StructField(\"historical_avg\", DoubleType(), True)\n",
    "    ])\n",
    "    \n",
    "    monitoring_log = spark.createDataFrame([(\n",
    "        datetime.datetime.now(),\n",
    "        critical_count,\n",
    "        ml_count,\n",
    "        providers_review,\n",
    "        email_sent,\n",
    "        freshness_hours,\n",
    "        volume_status,\n",
    "        current_volume,\n",
    "        historical_avg\n",
    "    )], monitoring_schema)\n",
    "    \n",
    "    monitoring_log.write.format(\"delta\").mode(\"append\")\\\n",
    "        .option(\"mergeSchema\", \"true\")\\\n",
    "        .saveAsTable(\"medisure_jen.audit.fraud_monitoring_log\")\n",
    "    \n",
    "    print(\"‚úÖ Audit log saved successfully\")\n",
    "    \n",
    "    # 4. Cleanup temp tables with error handling\n",
    "    temp_tables = [\n",
    "        \"medisure_jen.temp.critical_alerts\",\n",
    "        \"medisure_jen.temp.ml_anomalies\", \n",
    "        \"medisure_jen.temp.fraud_decision\",\n",
    "        \"medisure_jen.temp.email_status\",\n",
    "        \"medisure_jen.temp.alert_true_fraud_detected\",\n",
    "        \"medisure_jen.temp.alert_false_no_fraud\"\n",
    "    ]\n",
    "    \n",
    "    cleaned_count = 0\n",
    "    for table in temp_tables:\n",
    "        try:\n",
    "            spark.sql(f\"DROP TABLE IF EXISTS {table}\")\n",
    "            cleaned_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not drop table {table}: {e}\")\n",
    "    \n",
    "    # 5. Final summary\n",
    "    print(\"=\"*60)\n",
    "    print(\"AUDIT LOGGING COMPLETE\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"üìä Critical Alerts: {critical_count}\")\n",
    "    print(f\"ü§ñ ML Anomalies: {ml_count}\")\n",
    "    print(f\"üìß Email Sent: {email_sent}\")\n",
    "    print(f\"‚è∞ Data Freshness: {freshness_hours:.2f} hours\")\n",
    "    print(f\"üè• Providers Needing Review: {providers_review}\")\n",
    "    print(f\"üóëÔ∏è  Temp Tables Cleaned: {cleaned_count}/{len(temp_tables)}\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"üéØ Pipeline execution completed successfully!\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "# Execute main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "06_fraud_audit_logging",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
